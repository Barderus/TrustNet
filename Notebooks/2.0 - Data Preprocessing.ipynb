{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import all the necessary libraries",
   "id": "f9ed576a7784dd7f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-07T12:43:45.775563Z",
     "start_time": "2025-11-07T12:43:43.576334Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "import textstat\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:43:48.550627Z",
     "start_time": "2025-11-07T12:43:45.782917Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv(\"../data/preprocessed/master_fakenews.csv\")",
   "id": "dc3036c9bd999df6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now fix any contractions in the text and remove extra whitespace",
   "id": "a746b431883af81f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:43:48.575492Z",
     "start_time": "2025-11-07T12:43:48.568659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_text(text):\n",
    "    text = contractions.fix(str(text))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ],
   "id": "c97069fd70dd003",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:43:55.652020Z",
     "start_time": "2025-11-07T12:43:48.585714Z"
    }
   },
   "cell_type": "code",
   "source": "df[\"prep_text\"] = df[\"clean_text\"].apply(normalize_text)",
   "id": "6c9f81ed900a42f4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:43:55.672939Z",
     "start_time": "2025-11-07T12:43:55.666503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stop = set(stopwords.words(\"english\"))"
   ],
   "id": "609e9834dffda031",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To ensure that non-letter characters are still in the text, as well convert any leftover uppercase to lowercase, remove stopwords, and lemmatize",
   "id": "2db2a1f10895ff0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:43:55.683673Z",
     "start_time": "2025-11-07T12:43:55.678754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", str(text))\n",
    "    text = text.lower().split()\n",
    "    text = [wnl.lemmatize(word) for word in text if word not in stop]\n",
    "    return \" \".join(text)"
   ],
   "id": "4e033716222b46b2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:44:47.262044Z",
     "start_time": "2025-11-07T12:43:55.690661Z"
    }
   },
   "cell_type": "code",
   "source": "df[\"prep_text\"] = df[\"clean_text\"].apply(clean_text)",
   "id": "5b90a78521f67bdd",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now count unique words and compute the ratio of unique words to total text length This way we can analyze the lexical diversity of the text, it might be a clue on identifying fake news from real news.",
   "id": "275b61017b979229"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:44:49.246463Z",
     "start_time": "2025-11-07T12:44:47.291352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df[\"lexical_richness\"] = df.apply(\n",
    "    lambda row: row[\"unique_words\"] / row[\"word_count\"] if row[\"word_count\"] > 0 else 0,\n",
    "    axis=1\n",
    ")"
   ],
   "id": "6c6e3b9179a8a03a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's now calculate the Flesch Reading Ease and estimated grade level of each text\n",
    "- The Flesch Reading Ease is a readability test that scores text on a scale from 0 to 100, which shows how easy it's to read. Higher scores mean the text is easier to understand, with scores around 60-70 being suitable for most readers."
   ],
   "id": "9d7115cf33fcdfa9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:47:23.260322Z",
     "start_time": "2025-11-07T12:44:49.269286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df[\"flesch_reading_ease\"] = df[\"prep_text\"].apply(textstat.flesch_reading_ease)\n",
    "df[\"grade_level\"] = df[\"prep_text\"].apply(textstat.text_standard)"
   ],
   "id": "df9353b4e789ecf4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another part of the corpus that might give us some clue is the number of punctuaton and what's the ratio with words.",
   "id": "84eab2d4758e4aee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:47:23.329227Z",
     "start_time": "2025-11-07T12:47:23.324991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_punctuations(text):\n",
    "    \"\"\"Count punctuation characters in a text string.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return sum(1 for ch in text if ch in string.punctuation)"
   ],
   "id": "dfcdcff91de945dd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:47:23.353787Z",
     "start_time": "2025-11-07T12:47:23.349653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_words(text):\n",
    "    \"\"\"Count words in a text string (avoid division by zero).\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 1  # avoid division by zero\n",
    "    return len(text.split())"
   ],
   "id": "443eeae5d50b74f7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "31b35c2a00118b99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:47:30.745656Z",
     "start_time": "2025-11-07T12:47:23.373307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df[\"punct_count\"] = df[\"text\"].apply(count_punctuations)\n",
    "df[\"word_count\"]  = df[\"text\"].apply(count_words)\n",
    "df[\"punct_ratio\"] = df[\"punct_count\"] / df[\"word_count\"]"
   ],
   "id": "be391882c92b463e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:47:31.296580Z",
     "start_time": "2025-11-07T12:47:30.767862Z"
    }
   },
   "cell_type": "code",
   "source": "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])  # keep NER",
   "id": "18d572b92d7458a8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's use SpaCy to count the number of named entities in each text. NER give us the important entities in the text such as dates, monetary values, products, etc.",
   "id": "d233f7f92790feba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:29:22.093997Z",
     "start_time": "2025-11-07T12:47:31.301588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texts = df[\"prep_text\"].fillna(\"\").tolist()\n",
    "entity_counts = []\n",
    "\n",
    "for doc in nlp.pipe(texts, batch_size=100):\n",
    "    entity_counts.append(len(doc.ents))\n"
   ],
   "id": "7f5ed42bb9946dfe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\PycharmProjects\\TrustNet\\.venv\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:29:22.235901Z",
     "start_time": "2025-11-07T13:29:22.222467Z"
    }
   },
   "cell_type": "code",
   "source": "df[\"entity_count\"] = entity_counts",
   "id": "2700999ef2ff8a9a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert cleaned text into numerical TF-IDF features (1- and 2-grams)",
   "id": "cef4faf99161e79f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:30:03.270412Z",
     "start_time": "2025-11-07T13:29:22.247210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_tfidf = tfidf.fit_transform(df[\"prep_text\"])"
   ],
   "id": "8c39187f3795ecbc",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Merge TF-IDF matrix with numeric features like polarity, subjectivity, etc.",
   "id": "5c8da7853f1f5279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:30:03.421111Z",
     "start_time": "2025-11-07T13:30:03.304730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numeric_feats = df[[\"polarity\", \"subjectivity\", \"text_len\", \"avg_word_len\"]].fillna(0).values\n",
    "X_full = hstack([X_tfidf, numeric_feats])"
   ],
   "id": "382164b34afa44d6",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Identify which numeric features are most related to the label `real`",
   "id": "8b6e6c11858d7de8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:30:03.465995Z",
     "start_time": "2025-11-07T13:30:03.427117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corrs = df.corr(numeric_only=True)[\"real\"].sort_values(ascending=False)\n",
    "print(corrs)"
   ],
   "id": "23c6c8df8f8f904a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real                   1.000000\n",
      "lexical_richness       0.165905\n",
      "entity_count           0.163239\n",
      "avg_word_len           0.162847\n",
      "flesch_reading_ease   -0.008832\n",
      "polarity              -0.025019\n",
      "unique_words          -0.035003\n",
      "punct_ratio           -0.047456\n",
      "text_len              -0.054482\n",
      "word_count            -0.056424\n",
      "punct_count           -0.085365\n",
      "subjectivity          -0.317270\n",
      "num_sents                   NaN\n",
      "Name: real, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Looking at the extreme of the our list, lexical_richness has a very small correlation with our label, whereas subjectivity has a somewhat strong negative correlation with our label",
   "id": "3ef932f59a0847b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:30:11.608430Z",
     "start_time": "2025-11-07T13:30:03.471670Z"
    }
   },
   "cell_type": "code",
   "source": "df.to_csv(\"../data/preprocessed/fakenews_preprocessed.csv\", index=False)",
   "id": "e4a2e28b1506148e",
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
